# -*- coding: utf-8 -*-
import streamlit as st
from openai import OpenAI

# ==================== åˆå§‹åŒ–é…ç½® ====================
st.set_page_config(
    page_title="æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ",
    page_icon="ğŸ¤–",
    layout="centered"
)  # â€Œ:ml-citation{ref="2,4" data="citationList"}

# ==================== ä¾§è¾¹æ é…ç½® ====================
with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶é¢æ¿")
    api_key = st.text_input("APIå¯†é’¥", type="password")
    client = OpenAI(api_key=api_key) if api_key else None  # â€Œ:ml-citation{ref="2" data="citationList"}
    
    st.divider()
    temperature = st.slider("åˆ›é€ åŠ›", 0.0, 2.0, 0.7)
    max_tokens = st.number_input("æœ€å¤§è¾“å‡ºé•¿åº¦", 512, 4096, 2048)  # â€Œ:ml-citation{ref="2,4" data="citationList"}
    
    if st.button("ğŸ§¹ æ¸…ç©ºå¯¹è¯"):
        st.session_state.messages = []
        st.rerun()  # â€Œ:ml-citation{ref="3" data="citationList"}

# ==================== ä¸»ç•Œé¢ ====================
st.title("ğŸ’¬ æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ")
st.caption("æ”¯æŒGPT-4 Turboçš„æµå¼å¯¹è¯ç³»ç»Ÿ")

# åˆå§‹åŒ–æ¶ˆæ¯å®¹å™¨
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ"}
    ]  # â€Œ:ml-citation{ref="2,3" data="citationList"}

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    avatar = "ğŸ¤–" if msg["role"] == "assistant" else None
    with st.chat_message(msg["role"], avatar=avatar):
        st.markdown(msg["content"])  # â€Œ:ml-citation{ref="2,3" data="citationList"}

# ==================== å¯¹è¯å¤„ç† ====================
if prompt := st.chat_input("è¾“å…¥æ¶ˆæ¯..."):
    # ç”¨æˆ·æ¶ˆæ¯å¤„ç†
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)  # â€Œ:ml-citation{ref="2,3" data="citationList"}

    # AIå“åº”å¤„ç†
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        response = client.chat.completions.create(
            model="gpt-4-turbo",
            messages=st.session_state.messages,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=True  # â€Œ:ml-citation{ref="1,2" data="citationList"}
        )
        
        # æµå¼è¾“å‡ºå®ç°
        response_container = st.empty()
        full_response = ""
        for chunk in response:
            if chunk.choices.delta.content:
                full_response += chunk.choices.delta.content
                response_container.markdown(full_response + "â–Œ")  # â€Œ:ml-citation{ref="1,2" data="citationList"}
        
        response_container.markdown(full_response)
    
    st.session_state.messages.append(
        {"role": "assistant", "content": full_response}
    )  # â€Œ:ml-citation{ref="2,3" data="citationList"}
